# Загальні відомості:
"Парсинг сайтів" або "парсинг контенту" - це процес вилучення даних з будь-якого сайту в мережі Інтернет.

Типовим прикладом парсинга контенту є копіювання списку контактів з якогось веб-каталогу. Однак вилучення та збереження даних з веб-сторінки в таблицю Excel працює тільки з невеликими обсягами даних і займає чимало часу. Щоб обробити великі масиви даних, потрібна автоматизація. І тут в справу вступають веб-парсери.

Парсинг сайтів здійснюється за допомогою спеціальної програми "веб-парсера" або "бота" або "веб-павука" (зазвичай всі ці поняття використовуються як синоніми). Веб-парсер сканує веб-сторінки, завантажує контент, витягує з нього потрібні дані і потім зберігає їх в файлах або базі даних.

Парсинг сайтів може використовуватися для автоматизації різноманітних задач по збору даних. Веб-парсери разом з іншими програмами можуть робити практично все те ж саме, що робить людина в браузері і багато іншого. Вони можуть автоматично замовити вашу улюблену їжу, купити квитки на концерт, як тільки вони стануть доступні, періодично сканувати сайти електронної комерції і відправляти вам текстові повідомлення, коли ціна на товар знизиться, і т. д
Веб-парсер - це програма або скрипт, який використовується для завантаження контенту веб-сторінок (зазвичай тексту відформатованого на HTML) і вилучення з нього даних.

Веб-парсер сканує веб-сайти, витягує з ні дані, перетворює їх в зручний структурований формат і зберігає у файлі або базі даних для подальшого використання.
Які компоненти можуть бути в веб-парсері:

1. "Сфокусований" модуль веб-сканування

Модуль веб-сканера переміщається по цільовому веб-сайту, відправляючи HTTP або HTTPS запити на URL-адреси, слідуючи певним шаблоном або деякої логіці розбиття на сторінки. Сканер завантажує об'єкти відповіді у вигляді вмісту HTML і передає ці дані в екстрактор. Наприклад, сканер запуститься на сторінці з адресою https://example.com і просканує сайт, переходячи по посиланнях на головній сторінці.

2. Модуль вилучення (екстрактор) або аналізатор

Отриманий HTML обробляється з використанням синтаксичного аналізатора, який витягує необхідні дані з HTML в напівструктуровану форму. Існують різні методи розбору:

Регулярні вирази - набір регулярних виразів може використовуватися для пошуку за шаблоном під час обробки тексту в HTML даних. Цей метод корисний рішення для простих завдань, на зразок вилучення списку всіх електронних адрес на веб-сторінці. Але не підходить для вирішення більш складних завдань, таких як отримання різних полів на сторінці з описом товару на сайті електронної комерції. 
Аналіз HTML - це найбільш часто використовуваний метод аналізу даних з веб-сторінки. Більшість веб-сайтів спираються на якусь базу даних, з якої вони читають контент і створюють різні сторінки за однаковими шаблонами. HTML аналізатори перетворять код HTML в деревоподібну структуру, по якій можна переміщатися програмно з використанням напівструктурованих мов запитів.
Аналіз DOM з використанням повних або "безголових" (без візуального інтерфейсу) браузерів - Оскільки Інтернет перетворився в складні веб-додатки, які сильно залежать від JavaScript, простого завантаження веб-сторінки і коду HTML стало недостатньо. Такі сторінки динамічно оновлюють дані всередині браузера, не відправляючи вас на іншу сторінку . Завантажуючи HTML код таких веб-сторінок, ви отримуєте тільки зовнішню HTML оболонку веб-додатка. Вона буде містити тільки відносні посилання і не дуже релевантний контент або дані. Для таких веб-сайтів простіше використовувати повноцінний браузер, такий як Firefox або Chrome. Цими браузерами можна керувати за допомогою інструменту автоматизації браузера.
Дані, одержувані цими браузерами, можуть потім запитуватися за допомогою селекторів DOM..
Автоматичне вилучення з використанням штучного інтелекту - ця техніка складніша і в основному використовується, коли сканується кілька сайтів, які підпадають під певну вертикаль. Ви можете навчати веб-парсери, використовуючи моделі машинного навчання з вилучення даних. Наприклад, можна використовувати моделі розпізнавання іменованих об'єктів для отримання даних, таких як контактні дані, з просканованих веб-сторінок.

3. Модуль перетворення і очищення даних

Дані, витягнуті синтаксичним аналізатором, не завжди мають формат, який підходить для негайного використання. Більшість витягнутих наборів даних потребують тій чи іншій формі "очищення" або "перетворення". Для виконання цього завдання використовуються регулярні вирази, операції з рядками і методи пошуку.

Якщо веб-парсер отримує дані з невеликої кількості сторінок, то зазвичай вилучення та перетворення виконуються в одному модулі.

4. Модуль серіализації і збереження даних

Після отримання очищених даних їх необхідно серіалізіровать відповідно до заданих моделями даних. Це останній модуль, який виводить дані в стандартному форматі, який може зберігатися в базах даних (Oracle, SQL Server, MongoDB і т.д.), в файлах JSON / CSV або передаватися в сховища даних.

Є багато способів написати веб-парсер. Ви можете написати код з нуля для всіх перерахованих вище модулів або використовувати інтегровані середовища з абстрактними шарами цих модулів. Написання коду з нуля відмінно підходить для вирішення невеликих завдань по парсингу даних. Але як тільки парсинг виходить за рамки кількох різних типів веб-сторінок, краще скористатися фреймворком.

Крім цього, існують інструменти для парсинга веб-сторінок за допомогою візуального інтерфейсу, де ви можете задавати необхідні для отримання дані, і сервіс автоматично створить веб-парсер з цими інструкціями. Однак, подібні веб-інструменти знаходяться ще в сирому стані. Для більш менш складних завдань вам все ж доведеться написати код веб-парсинга самостійно.

## Що таке Headless Браузер

Вже з назви зрозуміло, що це щось без голови. В контексті браузера це означає наступне:

-У нього немає реальної рисовки вмісту, тобто він все відрисовує в пам'яті.
-За рахунок цього він споживає менше пам'яті, тому що не потрібно малювати картинки або гігабайтні PNG, які люди намагаються закласти в бекенд.
-Він працює швидше, бо йому нічого не потрібно відрисовувати на реальному екрані.
-Має програмний інтерфейс для управління. Ви запитаєте - у нього ж немає інтерфейсу, кнопочок, віконець? Як же їм управляти? Тому звичайно ж він має інтерфейс для управління
-Ще одна важлива властивість - можливість установки на «голий» Linux-сервер. Це потрібно для того, щоб, якщо у вас є свіжовстановлений Ubuntu або Red Hat, ви можете просто закинути туди бінарник або поставити пакет, і браузер працюватиме з коробки. Ніякого шаманства або вуду-магії не знадобиться.

Так схематично виглядає звичайною браузер на основі WebKit. Ви можете не вчитуватися в компоненти - це просто наочне зображення.

Нас цікавить тільки верхній компонент Browser UI. Це той самий інтерфейс користувача - віконця, меню, спливаючі повідомлення і все інше.

Так виглядає безголовий браузер. Помітили різницю? Ми повністю прибираємо інтерфейс користувача. Його більше немає. Залишається тільки браузер.



Засоби обробки на платформі Node JS:
Парсинг сайтів у переважній більшості випадків складається з трьох етапів:
Завантаження вмісту веб-сторінки;
Безпосередній парсинг і вилучення структурованих даних;
Збереження  даних в певному форматі або в базі даних.

Завантаження вмісту веб-сторінки:
Можна використовувати HTTP-клієнти, наприклад, axios або needle, які мають змогу надсилати HTTP-запити за вказаними адресами і отримувати вміст сторінок, тобто їхній HTML-код. Такий спосіб отримання вмісту підходить лише для сайті зі статичним вмістом, тобто який не змінюється при перезавантажені сторінки.

Але наразі сайти набувають більш динамічного характеру, тобто вміст веб-сайту відображається через JavaScript, наприклад, Twitter. В цьому випадку HTTP-клієнт завантажить HTML, який буде містити шляхи до скриптів для генерації, але не згенеровану сторінку. В таких випадках потрібно відтворити роботу браузера і користувача. Для цієї цілі розробили так звані “headless” браузери..
А для використання цих браузерів є інструменти веб-автоматизації такі як Selenium, Cypress, Nightmare, Puppeteer, x-ray.
Саме ці інструменти та бібліотеки зазвичай використовують тестувальники програмного забезпечення. З їх допомогою можна відкрити екземпляр браузера, на якому веб-сайт працює так само, як і в інших браузерах, і, відповідно, запустити файли JavaScript, які відображають динамічний вміст сайту. І потім можна буде отримати вже згенерований вміст.

Безпосередній парсинг і вилучення структурованих даних:
В Node JS найпопулярнішим засобом вилучення даних є бібліотека Cheerio.

Особливості:
Cheerio реалізує підмножину основного jQuery. Cheerio усуває всі невідповідності DOM і помилки браузера з бібліотеки jQuery, розкриваючи його справді чудовий API.

Cheerio працює з дуже простою, послідовною моделлю DOM. Як результат, синтаксичний аналіз, маніпулювання та рендеринг неймовірно ефективні.

Cheerio побудований над синтаксичним аналізатором parse5 і може додатково використовувати htmlparser2, який прощає помилки. Cheerio може аналізувати майже будь-який документ HTML або XML.

Cheerio - це не веб-браузер:
Cheerio аналізує розмітку та надає API для обходу / маніпулювання отриманою структурою даних. Він не інтерпретує результат як веб-браузер. Зокрема, він не виконує візуалізацію, не застосовує CSS, не завантажує зовнішні ресурси та не виконує скрипти JavaScript. Це робить Cheerio набагато швидшим, ніж інші рішення. 
Також за необхідності використання будь-якої з цих функцій можна розглянути такі проекти, як Puppeteer або JSDom.

Збереження  даних в певному форматі або в базі даних:
В Node JS є вбудований об’єкт JSON з функцією “stringify”, за допомогою якої можна будь-яку структуру даних перетворити в рядок і записати у файл.
